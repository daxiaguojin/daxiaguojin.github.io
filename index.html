<!DOCTYPE html>
<html lang="zh-Hant">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>GenAI 技術演進史</title>
    <script src="https://cdn.tailwindcss.com"></script>
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;700&family=Poppins:wght@400;600&display=swap" rel="stylesheet">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.5.2/css/all.min.css">
    <script src="https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.min.js"></script>
    <style>
        body { font-family: 'Poppins', sans-serif; background-color: #f8f7f2; }
        .font-noto { font-family: 'Noto Serif SC', serif; border-left: 16px solid #10bbbb; padding-left: 1rem;}
        .visual-anchor { font-size: 14rem; font-weight: 900; line-height: 0.8; color: transparent; -webkit-text-stroke: 2px #ef4444; z-index: -1; }
        .card-content { min-height: 800px; }
        .card-content h3 { font-family: 'Noto Serif SC', serif; font-size: 2.5rem; font-weight: 700; margin-bottom: 1rem; }
        .card-content h4 { font-family: 'Noto Serif SC', serif; font-size: 1.5rem; font-weight: 700; margin-top: 2rem; margin-bottom: 0.5rem; border-left: 4px solid #ef4444; padding-left: 1rem; }
        .card-content p, .card-content li { font-family: 'Poppins', 'Noto Serif SC', serif; font-size: 1.1rem; line-height: 2; margin-bottom: 1rem; }
        .card-content ul { list-style-position: inside; padding-left: 1rem; list-style-type: disc; }
        .card-content table { width: 100%; margin-top: 1rem; border-collapse: collapse; }
        .card-content th, .card-content td { font-family: 'Poppins', sans-serif; border: 1px solid #d1d5db; padding: 0.75rem; text-align: left; }
        .card-content th { background-color: #f3f4f6; font-weight: 600; }
        .card-content pre { background-color: #f3f4f6; padding: 1rem; border-radius: 0.5rem; overflow-x: auto; margin: 1rem 0; font-family: 'Courier New', Courier, monospace; white-space: pre-wrap; }
        .footer { font-family: 'Noto Serif SC', serif; font-size: 1.5rem; font-weight: 700; margin-top: 2rem; margin-bottom: 0.5rem; background: #10bbbb; padding-left: 1rem; margin: 0;}
        .diagram-placeholder { cursor: pointer; font-size: 3rem; color: #ef4444; transition: transform 0.2s; }
        .diagram-placeholder:hover { transform: scale(1.1); }
        .home-card-list li { font-size: 1.1rem; line-height: 1.8; margin-bottom: 0.5rem; }
        .home-card-list li a { cursor: pointer; transition: color 0.2s; }
        .home-card-list li a:hover { color: #ef4444; text-decoration: underline; }
        #mermaid-render-container svg { max-width: none; width: 100%; height: 100%; transition: transform 0.2s ease-in-out; transform-origin: 0 0; }
        #mermaid-render-container.pan-active { cursor: grab; }
        #mermaid-render-container.pan-active.panning { cursor: grabbing; }
        .mermaid { display: flex; justify-content: center; width:100%; height:100%; font-family: 'Poppins', sans-serif; font-size: 1.2rem; }
        #zoom-controls { padding-right: 20px; }
    </style>
</head>
<body class="text-gray-800">

    <header class="sticky top-0 z-30 w-full bg-white/95 backdrop-blur-sm border-b-2 border-gray-200 py-4 px-4">
        <div class="flex justify-between items-center mx-auto">
            <h1 class="font-noto text-3xl md:text-4xl font-bold whitespace-nowrap">GenAI應用技術演進史</h1>
            <div class="flex items-center space-x-2">
                <button id="start-journey" class="bg-red-500 text-white font-poppins font-semibold py-2 px-4 rounded-lg hover:bg-red-600 transition">開始演進之旅</button>
                <button id="prev-card" class="bg-gray-200 text-gray-700 font-poppins font-semibold py-2 px-4 rounded-lg hover:bg-gray-300 transition">上一張卡片</button>
                <button id="next-card" class="bg-gray-200 text-gray-700 font-poppins font-semibold py-2 px-4 rounded-lg hover:bg-gray-300 transition">下一張卡片</button>
                <button id="go-home" class="bg-gray-200 text-gray-700 font-poppins font-semibold py-2 px-6 rounded-lg hover:bg-gray-300 transition">回到首頁</button>
            </div>
        </div>
    </header>

    <main class="w-full mx-auto p-4">
        <div class="grid grid-cols-12 gap-8">
            <aside class="col-span-12 md:col-span-2 flex items-center justify-center relative">
                <div id="visual-anchor-container" class="visual-anchor"></div>
            </aside>
            <section id="info-cards-container" class="col-span-12 md:col-span-10 relative">
                <!-- Dynamically generated cards will be inserted here -->
            </section>
            <aside class="col-span-12 md:col-span-2 flex items-center justify-center relative">
            </aside>
            <section id="info-cards-container" class="col-span-12 md:col-span-10 relative">
                <div class="footer p-10 shadow-lg rounded-lg"> 
                    <h4>此時此刻，AI的演進還在繼續中...(未完待續)</h4>
                </div>
            </section>
        </div>
    </main>

    <div id="mermaid-modal" class="fixed inset-0 bg-black bg-opacity-75 flex items-center justify-center p-4 hidden z-50">
        <div id="modal-overlay" class="absolute inset-0"></div>
        <div class="bg-white rounded-lg shadow-2xl p-6 relative w-11/12 h-5/6 flex flex-col">
            <button id="modal-close" class="absolute top-4 right-4 text-gray-500 hover:text-gray-800 text-2xl z-10">&times;</button>
            <div class="flex justify-between items-center mb-4 flex-shrink-0">
                <h3 id="modal-title" class="font-noto text-2xl font-bold">流程圖</h3>
                <div id="zoom-controls" class="flex items-center space-x-2">
                    <button id="zoom-out" class="p-2 bg-gray-200 rounded-md hover:bg-gray-300 transition-colors"><i class="fas fa-search-minus"></i></button>
                    <button id="zoom-in" class="p-2 bg-gray-200 rounded-md hover:bg-gray-300 transition-colors"><i class="fas fa-search-plus"></i></button>
                    <button id="zoom-reset" class="p-2 bg-gray-200 rounded-md hover:bg-gray-300 transition-colors"><i class="fas fa-expand"></i></button>
                    <button id="pan-toggle" class="p-2 bg-gray-200 rounded-md hover:bg-gray-300 transition-colors"><i class="fas fa-arrows-alt"></i></button>
                    <button id="download-svg" class="p-2 bg-gray-200 rounded-md hover:bg-gray-300 transition-colors"><i class="fas fa-file-code"></i></button>
                    <button id="download-png" class="p-2 bg-gray-200 rounded-md hover:bg-gray-300 transition-colors"><i class="fas fa-file-image"></i></button>
                </div>
            </div>
            <div id="mermaid-render-container" class="flex-grow flex justify-center items-center bg-gray-100 p-4 rounded-md overflow-auto"></div>
        </div>
    </div>

    <script>
        document.addEventListener('DOMContentLoaded', () => {
            mermaid.initialize({ startOnLoad: false, theme: 'neutral' });

            const cardData = [
                {
                    id: 1,
                    anchor: '1',
                    title: 'GPT-3/3.5',
                    subtitle: '2020.06~2022.11',
                    body: `<p>OpenAI 於 2020 年 6 月 發佈 GPT-3，標誌著大型語言模型（LLM）的重要進展。1750億參數展現前所未有的語言理解和生成能力。</p><p>OpenAI於 2022 年 推出 GPT‑3.5，包括 text-davinci-002/003 及 gpt-3.5‑turbo，其中 ChatGPT 正是基於 GPT‑3.5 的 fine‑tuned 版本（即 InstructGPT 系列）。</p><p>ChatGPT於 2022 年 11 月推出，僅 5 天即突破 100 萬用戶，2 個月內達到 1 億用戶，顯著改變全球對 AI 的認知與採用節奏 。</p><h4>GPT 系列模型發布時間表</h4><table class="w-full text-left border-collapse"><thead><tr><th>發佈時間</th><th>模型</th><th>參數</th></tr></thead><tbody><tr><td>2018</td><td>GPT-1 發佈</td><td>1.17 億個參數</td></tr><tr><td>2019</td><td>GPT-2 發佈</td><td>最大 15 億個參數</td></tr><tr><td>2020</td><td>GPT-3 發佈</td><td>1750 億個參數</td></tr><tr><td>2022-11-30</td><td>GPT-3.5 發佈</td><td>驅動 ChatGPT</td></tr><tr><td>2023-03</td><td>GPT-4 發佈</td><td>提升推理能力與穩定性</td></tr><tr><td>2023-11</td><td>GPT-4 Turbo</td><td>效能/成本優化，context 最長 128K</td></tr><tr><td>2024-05</td><td>GPT-4o 發佈</td><td>多模態整合 : 圖像、語音、影片</td></tr></tbody></table>`,
                    mermaid: null
                },
                {
                    id: 2,
                    anchor: '2',
                    title: '早期大語言模型 Early Large Language Models',
                    subtitle: '~2022',
                    body: `<p>早期的 LLM（如 GPT-2, GPT-3）能夠生成類似人類的自然語言文本，但存在幾個主要問題：</p><ul><li>知識受訓練數據限制：它們只能回答基於訓練數據的問題，無法即時更新知識。</li><li>幻覺（Hallucination）問題：經常生成錯誤或不存在的資訊。</li><li>缺乏行動能力：它們只能提供文本回應，不能與外部系統互動。</li></ul><h4>流程：</h4><p>在早期 LLM 應用中，用戶與模型之間的交互是單向且靜態的：</p><ul><li>用戶輸入:提供問題或指令</li><li>LLM 處理:根據訓練數據生成回應</li><li>靜態輸出:返回文本結果，並無與外部系統互動</li></ul>`,
                    mermaid: `flowchart LR\n    USER((User)) ------>| Query | LLM((LLM))\n    LLM ------>| Response | USER\n\nstyle USER strostroke-width:4px,color:#000,fill:#EC2D7A\nstyle LLM strostroke-width:4px,color:#FFF,fill:#EF4444`
                },
                {
                    id: 3,
                    anchor: '3',
                    title: '提示工程 Prompt Engineering',
                    subtitle: '2022~',
                    body: `<h4>發現的問題：</h4><ul><li>LLM雖然功能強大，但回應品質極度依賴使用者的輸入。</li><li>初期使用者經常無法得到預期的回答，原因可能是輸入模糊、不具體、或缺乏上下文說明。</li><li>LLM的回應常常出現錯誤、幻覺，或不符合商業應用場景的需求。</li></ul><h4>解決方案：Prompt Engineering（提示詞工程）</h4><p>提示詞工程的基本概念與原理：透過設計更明確、有結構的提示詞（Prompt），來引導語言模型產生更精確、穩定、有用的回應。這些提示詞可包括範例、角色設定、格式限制、步驟指令等，引導 LLM 「如何思考與回答」。</p><h4>改進點：</h4><ul><li>提升 LLM 回應的一致性與可預測性。</li><li>可針對不同任務（摘要、分類、問答、轉換格式等）設計不同的 prompt 模板，提高應用彈性與品質。</li><li>可結合「Chain-of-Thought（思路鏈）」、「Zero-shot/One-shot/Few-shot Examples」等技巧，提升推理與問題解決能力。</li></ul><h4>流程：</h4>`,
                    mermaid: `flowchart TD\n    USER((User)) --> |Query:「三角形內角和是多少？」| COMPOSE{{Compose Prompt}}\n\n    subgraph PE[Prompt Engineering]\nZERO[Zero‑Shot CoT]\nSYSTEM[System Instructions]\nCONTEXT[Context Injection]\nFEWSHOT[Few‑shot Examples]\nZERO --> COMPOSE\nSYSTEM --> COMPOSE\nCONTEXT --> COMPOSE\nFEWSHOT --> COMPOSE\n    end\n\n    COMPOSE --> LLM((LLM))\n    LLM --> |Response: 邏輯思維 + 最終答案| USER\n\n    subgraph Examples\nZERO_EX[Zero‑Shot CoT:\n「三角形內角和是多少？ 請詳細分步驟解析之。」] --> ZERO\nSYSTEM_EX[系統指令:\n「你是一位資深數學教師，解答需邏輯清晰、步驟完整」] --> SYSTEM\nCONTEXT_EX[上下文:\n三角形知道內角和為 180°，問三角形的角推理。] --> CONTEXT\nFEW_EX[Few‑Shot CoT 示例:\nQ: 四邊形內角和？ \nA: …步驟… 最終答案\nQ: 三邊形內角和？ \nA: …步驟… 180°] --> FEWSHOT\n    end\n\nstyle USER strostroke-width:4px,color:#000,fill:#EC2D7A\nstyle LLM strostroke-width:4px,color:#FFF,fill:#EF4444`
                },
                {
                    id: 4,
                    anchor: '4',
                    title: 'Token',
                    subtitle: '2020~',
                    body: `<h4>甚麼是Token？：</h4><p>token 是現代大型語言模型（LLM）處理文本的基本單位。它不是固定的「字」或「詞」，而是由模型 tokenizer 自動分割出具有意義的「字串」或「子詞單位」。</p><ul><li>英文中的 "unbelievable" 可能被斷成 ["un", "believ", "able"]，所以一個字可能是多個 token。</li><li>中文通常每個字是一個 token，例如「測」、「試」、「中」分別為獨立 tokens。</li></ul><h4>對話的完整上下文包含：</h4><ul><li>使用者的提問</li><li>模型之前的回應</li><li>系統訊息（如角色設定）</li><li>你可能傳的文件、表格、指令等外部內容</li><li>歷史對話記錄（多輪內容）</li><li>模型之後的回應</li></ul><h4>當總 token 數超過限制時，模型會：</h4><ul><li>裁切最早的歷史對話內容（通常是你最早的提問與模型的回覆）</li><li>保留最近的幾輪對話 + 當前提問 + 系統訊息</li><li>這會導致模型「忘記你先前說的話」，可能讓回答變得前後不一致。</li></ul>`,
                    mermaid: `timeline\n    title 最大 Token 上限數\n\n    GPT-3.5 : 4k ~ 16k tokens : 最多 24 頁\n    GPT-4.1 : ≈1,000k tokens : 約 1,500–2,000 頁\n    Claude 4（Sonnet 4 / Opus 4）: 200k ~ 500k tokens : 約 300–500 頁\n    Gemini 2.5 Pro : 1,000k tokens : 約 1,500–2,000 頁`
                },
                {
                    id: 5,
                    anchor: '5',
                    title: '檢索增強生成 Retrieval-Augmented Generation (RAG)',
                    subtitle: '2021~',
                    body: `<h4>發現的問題：</h4><ul><li>訓練好的 LLM 雖然能生成自然語言，但知識是靜態的，無法即時查詢最新資訊或公司內部資料。</li><li>如果訓練資料中缺少某些領域的知識，模型會給出錯誤答案或胡亂編造。</li></ul><h4>解決方案：RAG（Retrieval-Augmented Generation，檢索增強生成）</h4><ul><li>原理：在 LLM 生成回應前，先從外部知識庫（如向量資料庫、企業內部文件）檢索相關資訊，並將這些資訊傳給 LLM 作為上下文來生成更準確的回應。</li></ul><h4>改進點：</h4><ul><li>讓 LLM 可以即時訪問外部知識，減少幻覺。</li><li>企業可以利用自己的資料（如技術文件、客服對話）來增強 LLM 的回答能力。</li></ul><h4>流程：</h4><ul><li>原始文檔 (string) → 使用 tokenizer 統計 token 長度 → chunking (使用 token count 作為切分邊界 + overlap) → 每個 chunk 啟用 tokenizer → 生成真正的 token ID 列表 → 再 embed 做語意向量。</li><li>而使用者的提問流程則是 Tokenizing (分詞) → Embedding (向量化)，最後再進行 retrieval 和 generation。</li></ul>`,
                    mermaid: `graph LR\n\n    %% --- Knowledge Base (Offline) ---\n    subgraph KB[Knowledge Base]\nPDF[PDF Files]\nPPTX[Office Files]\nCONF[Confluence Pages]\n    end\n\n    subgraph INGESTION[Embedding → Vector DB]\ndirection LR\nPDF --> CHUNK{{Chunk + Embed}}\nPPTX --> CHUNK\nCONF --> CHUNK\nCHUNK --> VDB[(Vector DB)]\n    end\n\n    %% --- User Query Flow (Online RAG) ---\n    USER((User)) -->|Query| EQ{{Embed Query}}\n    EQ --> VDB\n    VDB --> TOPK[Top-k Similarity Search]\n    TOPK --> PROMPT{{Build Prompt}}\n    PROMPT --> LLM((LLM))\n    LLM -->|Response| USER\n\nstyle USER strostroke-width:4px,color:#000,fill:#EC2D7A\nstyle LLM strostroke-width:4px,color:#FFF,fill:#EF4444`
                },
                {
                    id: 6,
                    anchor: '6',
                    title: '上下文學習 Context Learning',
                    subtitle: '2022~',
                    body: `<h4>發現的問題：</h4><ul><li>LLM 雖然具備廣泛知識，但缺乏對特定企業情境的理解。</li><li>無法長期記憶先前對話，也無法處理大量文件（受限於 token 上限）。</li></ul><h4>解決方案：</h4><ul><li>在 LLM 生成回應時，將相關資訊（例如使用者上傳的文件、對話上下文）一併作為 prompt 提供給模型參考，讓 LLM「彷彿」當下就學會這些資訊。</li></ul><h4>改進點：</h4><ul><li>讓 LLM 在不需要重新訓練模型的情況下，也能理解特定情境或專案資料。</li><li>適合中小型應用，或一次性處理文件（如合約審閱、報告摘要）。</li><li>開發速度快、門檻低，是許多 Copilot 工具（如 Office、VS Code）常見的技術基礎。</li></ul><h4>流程：</h4>`,
                    mermaid: `flowchart TD\n    subgraph ICL[In-Context Learning]\ndirection TB\n\nINST[Instruction Prompting<br/>Zero-shot] --> LLM1((LLM))\nFEWSHOT[Few-shot Prompting<br/>Examples] --> LLM2((LLM))\nSTUFFING[Prompt Stuffing<br/>Context Injection] --> LLM3((LLM))\n    end\n\n    USER((User)) ------> PROMPTING[Prompt Engineering]\n    PROMPTING ------>|Compose Prompt| ICL\n\n    LLM1 -->|Response| USER\n    LLM2 -->|Response| USER\n    LLM3 -->|Response| USER\n\n    subgraph Prompt_Stuffing_Context [Prompt Stuffing Details]\nDOCS[Documents / Tables / Knowledge] --> STUFFING\n    end\n\n    USER --> Prompt_Stuffing_Context\n\nstyle USER strostroke-width:4px,color:#000,fill:#EC2D7A\nstyle LLM1 strostroke-width:4px,color:#FFF,fill:#EF4444\nstyle LLM2 strostroke-width:4px,color:#FFF,fill:#EF4444\nstyle LLM3 strostroke-width:4px,color:#FFF,fill:#EF4444`
                },
                {
                    id: 7,
                    anchor: '7',
                    title: '函數調用 Function Calling',
                    subtitle: '2023.06',
                    body: `<h4>發現的問題：</h4><ul><li>LLM 可以產生正確的建議（例如「你可以申請一張請購單」），但它無法執行具體的動作（如查詢某一張請購單進度，或將待簽核的表單填寫意見核准送出）。</li><li>使用者仍然需要手動操作系統，降低了自動化效率。</li></ul><h4>解決方案：</h4><ul><li>OpenAI 於 2023 年 6 月 在其 API 中引入此功能，讓 LLM 可以根據需求輸出結構化數據（如 JSON），並透過 API 調用外部系統來執行具體任務。</li></ul><h4>改進點：</h4><ul><li>讓 LLM 可以即時訪問外部知識，減少幻覺。</li><li>企業可以利用自己的資料（如儲存在關聯式資料庫的資料、客服對話）來增強 LLM 的回答能力。</li></ul><h4>流程：</h4>`,
                    mermaid: `flowchart LR\n\n    USER((User)) -->|Query| LLM((LLM))\n    LLM --> CHECK{Need Function Call?}\n\n    CHECK -->|No| RESPONSE[Final Response]\n    CHECK -->|Yes| JSON{{Generate JSON Instructions}}\n\n    subgraph FC[Function Calling]\nJSON --> FUNCTION{{Call Tool / API}}\nFUNCTION --> EXECUTE{{Execute Function}}\n    end\n\n    EXECUTE -->|Execution Result| LLM\n    LLM -->|Response| USER\n    RESPONSE --> USER\n\nstyle USER strostroke-width:4px,color:#000,fill:#EC2D7A\nstyle LLM strostroke-width:4px,color:#FFF,fill:#EF4444`
                },
                {
                    id: 8,
                    anchor: '8',
                    title: '智能代理 AI Agent',
                    subtitle: '2023-24',
                    body: `<h4>發現的問題：</h4><ul><li>Function Calling 讓 LLM 可以執行單個指令，但它無法自主計劃和執行複雜任務。例如：「幫我整理一份 IT 年度報告」涉及多個步驟（查詢數據 → 生成摘要 → 匯總成報告）。</li><li>Function Calling 只能處理單一 API 呼叫，無法自己決定「下一步該做什麼」。</li></ul><h4>解決方案：</h4><ul><li>讓 LLM 不只是回答問題，而是根據目標「規劃步驟」、「決策下一步行動」並「執行多個函數」。在人工智慧領域，智能代理的概念早已存在，但結合 LLM 的 Agent 系統在 2023 年 開始受到廣泛關注。</li></ul><h4>改進點：</h4><ul><li>讓 AI 可以自主處理複雜任務，而不只是單次回應。</li><li>可以與 Function Calling 和 RAG 結合，形成完整的智能工作流。</li></ul><h4>流程：</h4>`,
                    mermaid: `flowchart TD\n\n    USER((User)) -->|Goal / Task| AGENT((AI Agent))\n\n    subgraph Agent Loop\nAGENT --> PLAN{{Plan Next Action}}\nPLAN --> ACT{Action Type?}\nACT -->|Tool| TOOL{{Tool / API Call}}\nACT -->|Reflect| THINK{{Internal Reflection}}\nACT -->|Ask| ASKUSER{{Ask User Clarification}}\nTOOL --> OBSERVE{{Observe Result}}\nTHINK --> OBSERVE\nASKUSER --> USER\nOBSERVE --> MEMORY{{Update Memory}}\nMEMORY --> PLAN\n    end\n\n    AGENT -->|Final Response / Completion| USER\n\nstyle USER strostroke-width:4px,color:#000,fill:#EC2D7A\nstyle AGENT strostroke-width:4px,color:#FFF,fill:#EF4444`
                },
                {
                    id: 9,
                    anchor: '9',
                    title: '代理式檢索增強生成 Agentic RAG',
                    subtitle: '2024',
                    body: `<h4>技術特點：</h4><ul><li>作為 RAG 的進化版本，Agentic RAG 在 2024 年 開始出現，結合 Agent 能力，使檢索與生成過程更加自主和靈活。</li></ul><h4>改進點：</h4><ul><li>能夠動態調整檢索策略，自主決定何時檢索、檢索什麼內容，以及如何整合檢索到的資訊，提供更智能的知識增強生成能力。</li></ul><h4>流程：</h4>`,
                    mermaid: `flowchart TD\n    USER((User)) -->|Query / Task| AGENT((AI Agent))\n\n    subgraph Agent Workflow\nAGENT --> PLAN{{Plan Step}}\nPLAN --> ACTION{Need External Knowledge?}\n\nACTION -->|Yes| RETRIEVE{{Retrieve Documents}}\nRETRIEVE --> EMBEDDING[Vector Similarity Search]\nEMBEDDING --> CONTEXT[Relevant Context]\n\nACTION -->|No| THINK{{Use Internal Reasoning}}\n\nCONTEXT --> AUGMENT[Augment Prompt]\nTHINK --> AUGMENT\n\nAUGMENT --> LLM((LLM))\n\nLLM --> TOOL_CHECK{Tool Call Needed?}\nTOOL_CHECK -->|Yes| TOOL_EXEC{{Call Tool/API}}\nTOOL_EXEC --> TOOL_RESULT[Tool Result]\nTOOL_RESULT --> PLAN\n\nTOOL_CHECK -->|No| RESPONSE[Final Response]\n    end\n\n    RESPONSE --> AGENT\n    AGENT -->|Response| USER\n\nstyle USER strostroke-width:4px,color:#000,fill:#EC2D7A\nstyle LLM strostroke-width:4px,color:#FFF,fill:#EF4444\nstyle AGENT strostroke-width:4px,color:#FFF,fill:#EF4444`
                },
                {
                    id: 10,
                    anchor: '10',
                    title: '模型上下文協議 Model Context Protocol (MCP)',
                    subtitle: '2024~25',
                    body: `<h4>發現的問題：</h4><ul><li>在大型企業應用中，通常需要多個 AI 模型或和外部系統協作，例如：<ul><li>一個 RAG 系統負責檢索企業內部文件。</li><li>一個 Function Calling 負責執行業務操作。</li><li>一個 Agent 負責綜合決策並指導整個流程。</li></ul></li><li>這些 AI 模型和外部系統各自獨立，沒有標準化的方式來共享上下文資訊，導致資訊孤島。</li></ul><h4>解決方案：</h4><ul><li>由 Anthropic 在 2024 年底推出的開放標準，它徹底改變了 AI 如何與「外部世界」互動的方式。你可以把它視為「AI 應用程式的 USB-C 接口」一種通用的連接方式，讓 AI 模型能安全、標準化地存取外部即時資料與工具。</li></ul><h4>改進點：</h4><ul><li>讓不同的 AI 模型互通，不再是獨立運行的「黑箱」。</li><li>讓 AI 生態系統更加靈活，能夠跨系統處理更複雜的任務。</li></ul><h4>流程：</h4>`,
                    mermaid: `flowchart TD\n    USER((User)) -->|Query| LLM((LLM))\n\n    LLM --> CHECK{Need context or tool?}\n    CHECK -->|No| RESPONSE[Final Response]\n    CHECK -->|Yes| MCP_CLIENT[MCP Client]\n\n    MCP_CLIENT -->|Prompt/Tool Request| MCP_SERVER[MCP Server]\n\n    subgraph Server Process\nCONTEXT_MANAGER[Context Manager]\nTOOL_REGISTRY[Tool Registry / Schema]\nTOOL_EXECUTOR[Tool Executor]\n\nMCP_SERVER --> CONTEXT_MANAGER\nCONTEXT_MANAGER --> MEMORY_DB[(Long-Term Memory)]\nCONTEXT_MANAGER --> VECTOR_DB[(Vector DB)]\nCONTEXT_MANAGER -->|Compose Prompt| FINAL_PROMPT{{Final Prompt}}\nFINAL_PROMPT --> MODEL((LLM))\nMODEL -->|Response or ToolCall| MCP_SERVER\nMCP_SERVER --> TOOL_REGISTRY\nTOOL_REGISTRY --> TOOL_EXECUTOR\nTOOL_EXECUTOR --> TOOL_RESULT[Result]\nTOOL_RESULT --> MCP_SERVER\n    end\n\n    MCP_SERVER -->|Return Output| MCP_CLIENT\n    MCP_CLIENT -->|Return Decision or Augmented Prompt| LLM\n    LLM --> RESPONSE\n    RESPONSE --> USER\n\nstyle USER strostroke-width:4px,color:#000,fill:#EC2D7A\nstyle LLM strostroke-width:4px,color:#FFF,fill:#EF4444\nstyle MODEL strostroke-width:4px,color:#FFF,fill:#EF4444`
                },
                {
                    id: 11,
                    anchor: '11',
                    title: '多代理系統 Multi-Agent System',
                    subtitle: '2024',
                    body: `<h4>技術特點:</h4><ul><li>多代理系統的概念早已有之，但在 AI 領域，結合 LLM 的多代理協作系統在 2024 年 開始被廣泛研究和應用。</li></ul><h4>應用方式:</h4><ul><li>多個AI代理分工合作，各司其職，協同解決複雜問題，展現集體智慧的力量。每個代理可以專精於不同的任務領域，通過協作完成單一代理無法處理的複雜工作。</li></ul>`,
                    mermaid: null
                },
                {
                    id: 12,
                    anchor: '12',
                    title: 'AI 輔助開發 AI‑Assisted Software Development',
                    subtitle: '2025',
                    body: `<h4>背景和問題：</h4><ul><li>隨著程式愈來愈複雜，開發者不僅要寫程式，還要 Debug、測試、維護 legacy code。</li></ul><h4>解決方案：</h4><ul><li>AI 如 VS Code (內建GitHub Copilot，或外掛Gemini Code Assist、Cline、Roo Code等)、Cursor、Windsurf、Trae等IDE工具，可在撰寫程式時自動補全 code、生成測試範例、偵錯建議、撰寫註解，提升效率並減少 bug。</li><li>這些AI工具也不限於需要使用IDE才能使用。包括Claude (Claude Code)、Google (Gemini)、Atlassian (Rovo Dev)、Amazon (Kiro)、Qwen等都已釋出在命令列可以執行的AI輔助開發工具。</li></ul><h4>例子：</h4><ul><li>補全：你寫 for (int i=0; i<10; i++)，AI 即刻建議整個迴圈內容。</li><li>偵錯：AI 主動指出潛在 Null pointer，並標記風險行數。</li></ul>`,
                    mermaid: null
                },
                {
                    id: 13,
                    anchor: '13',
                    title: '氛圍編程 Vibe Coding',
                    subtitle: '2025',
                    body: `<h4>技術特點:</h4><ul><li>這個概念源自 OpenAI 聯合創辦人 Andrej Karpathy，在 2025 年提出，用來描述一種「幾乎忘記程式碼存在，只用語意導引 AI 寫程式」的新開發方式 。</li><li>它與「No‑code／Low‑code」不同，強調『自然語言 → AI 生成邏輯和介面』，開發者不需了解語法，只需透過描述讓 AI 完成大部分程式生成 。</li></ul><h4>技術特點:</h4><ul><li>建立在大型語言模型（如 GPT‑4、Claude、Gemini、DeepSeek、Qwen等）可理解並生成程式碼之能力。</li></ul><h4>應用方式:</h4><ul><li>幫助非程式背景者「自己作原型、小工具」。</li></ul>`,
                    mermaid: null
                },
                {
                    id: 14,
                    anchor: '14',
                    title: '規格先行 Vibe Spec',
                    subtitle: '2025',
                    body: `<h4>技術特點:</h4><ul><li>Vibe Spec（或稱 Vibespec）是一種將自然語言與正式規格結合的方式，讓 LLM 在生成程式碼前，先產生一份清晰、可追蹤的規格文件。這有助於避免隨性生成導致的混亂與錯誤。</li></ul><h4>解決的問題：</h4><ul><li>Vibe Coding 雖快速流暢，但容易缺乏結構、可測試性，並帶來安全與維護風險。</li><li>Vibe Spec 可以為每個功能定義需求、輸入/輸出、流程與測試標準，讓 AI 輸出的程式碼更穩健且可複製。</li></ul><h4>例子：</h4><ul><li>工具層級：在絕大多數的AI輔助開發工具中，你都可以設定規則或透過提示詞讓 AI 在開始寫程式前，自動先生成 spec，而不是直接生成程式碼。</li><li>實務流程：使用三份 Markdown 文件（requirements.md、design.md、tasks.md）來描述使用者故事、架構設計與任務拆解，類似 Kiro Spec 的開發流程，確保 AI 按 spec 編碼。</li></ul><h4>流程：</h4>`,
                    mermaid: `flowchart TD\n  USER((User)) --> |啟動 Spec 模式| START[Spec Session]\n\n  subgraph Spec Workflow\n    START --> LLM((LLM))\n    LLM --> RA{{Define Requirements → requirements.md}}\n    RA --> DS{{Design System → design.md}}\n    DS --> TS{{Plan Tasks → tasks.md}}\n    TS --> EXEC{{Execute Tasks & Monitor Progress}}\n    EXEC --> DONE[功能完成]\n  end\n\n  subgraph Files \n    RA --> REQ[requirements.md（EARS 規範：WHEN … THE SYSTEM SHALL …）]\n    DS --> DES[design.md（系統架構圖／接口／資料流程）]\n    TS --> TAS[tasks.md（待辦清單、任務拆解、狀態追蹤）]\n  end\n\n  subgraph Auto Generation & Feedback\n    REQ --> TOOL_AI{{AI 製作需求草稿}}\n    TOOL_AI --> RA\n    DES --> TOOL_AI\n    TAS --> TOOL_AI\n    EXEC --> TOOL_AI\n  end\n\nstyle USER strostroke-width:4px,color:#000,fill:#EC2D7A\nstyle LLM strostroke-width:4px,color:#FFF,fill:#EF4444`
                },
                {
                    id: 15,
                    anchor: '15',
                    title: '多模態人工智能 Multimodal AI',
                    subtitle: '2025',
                    body: `<h4>背景和問題：</h4><ul><li>單純文本或圖像的思維限制創意與應用場景。</li></ul><h4>解決方案：</h4><ul><li>多模態模型能處理文字／圖片／語音／影音，讓應用更貼近複雜需求。</li></ul><h4>例子：</h4><ul><li>客服 AI 能根據客戶上傳的故障照片，分析問題並語音回覆操作步驟。</li><li>企劃中 AI 幫你根據腳本直接生成簡短動畫提案片段。</li></ul>`,
                    mermaid: null
                },
                {
                    id: 16,
                    anchor: '16',
                    title: '合成式多媒體 Synthetic Media',
                    subtitle: '2025',
                    body: `<h4>背景和問題：</h4><ul><li>傳統影像、音頻內容製作耗時高、人力密集。</li></ul><h4>解決方案：</h4><ul><li>GenAI可自動生成圖像、音樂、影片、甚至互動遊戲元素。</li></ul><h4>例子：</h4><ul><li>藝術家用 AI 生成獨特的數位藝術作品。</li><li>影片製作者用 AI 生成動畫角色和背景。</li><li>音樂人用 AI 生成背景音樂或聲效。</li></ul>`,
                    mermaid: null
                },
                {
                    id: 17,
                    anchor: '17',
                    title: '生成設計 Generative Design',
                    subtitle: '2025',
                    body: `<h4>背景和問題：</h4><p>工程或建築設計需考慮節能、成本、材質等複雜參數。</p><h4>解決方案：</h4><ul><li>AI 把 CAD + 環境數據導入，經由演算法生成多種設計方案供選擇。</li></ul><h4>例子：</h4><ul><li>建築師需求：只描述「需要最大自然採光、用最低成本建一棟辦公室」，AI 畫出 5 種外觀與材料配置供選擇。</li><li>IT 機櫃規劃：規劃機櫃配置時，AI 根據冷度流量與電力負載給出合理排佈建議。</li><li>PTC的Creo：工程師可以透過互動方式指定各自的需求與目標，包括偏好的材料與製造流程，而生成式設計引擎就會自動產生可立即生產的設計，以便當做繼續設計的起點或最終版本的解決方案。因此工程師可以與技術互動，更快建立卓越的設計並推動產品創新。</li></ul>`,
                    mermaid: null
                },
                {
                    id: 18,
                    anchor: '18',
                    title: '因果人工智慧 Causal AI',
                    subtitle: '2025',
                    body: `<h4>背景和問題：</h4><ul><li>GenAI背後的大型語言模型雖然可以用類似人類對話的方式回答問題，甚至創建連貫且富有創意的文字，但 LLM 的回答內容事實上是以統計學上最有可能出現的字詞，而非像是人類在思考般以「因果」進行推論的。</li><li>世界經濟論壇報導提出，將「因果AI」與生成式AI 結合，將能輔助生成式AI 提升決策解釋力、減少偏差與風險。</li></ul><h4>解決方案：</h4><ul><li>因果 AI 建立「X 導致 Y」模型，幫企業做更可靠的決策。</li></ul><h4>例子：</h4><ul><li>IT 評估系統升級是否導致效能下降，就能區分「版本不穩定導致的問題」vs.「正確升級帶來的延遲」。</li><li>客服部門可找出「培訓完客服是否真正改善滿意度」，而不是只看時間關聯。</li></ul>`,
                    mermaid: null
                },
                {
                    id: 19,
                    anchor: 'R1',
                    title: '問題R1',
                    subtitle: '',
                    body: `<h4>要怎麼讓已整合RAG技術的GenAI只回答搜尋到的文件內容，而不會亂說話？</h4>`,
                    mermaid: null
                },
                {
                    id: 20,
                    anchor: 'M1',
                    title: '問題M1',
                    subtitle: '',
                    body: `<h4>在剛剛講述的應用技術中，用那一種技術可以來查詢BPM資料庫中的資訊？</h4>`,
                    mermaid: null
                },
                {
                    id: 21,
                    anchor: 'P1',
                    title: '問題P1',
                    subtitle: '',
                    body: `<h4>那些應用技術是可以來解決GenAI產生幻覺？</h4>`,
                    mermaid: null
                },
                {
                    id: 22,
                    anchor: 'A1',
                    title: '問題A1',
                    subtitle: '',
                    body: `<h4>AI輔助開發工具用了那些我們剛剛講述的應用技術？</h4>`,
                    mermaid: null
                },
                {
                    id: 23,
                    anchor: 'F1',
                    title: '問題F1',
                    subtitle: '',
                    body: `<h4>ChatGPT聊天介面上可以開啟的網頁搜尋功能，你覺得是使用哪一種應用技術達成？</h4>`,
                    mermaid: null
                },
                {
                    id: 24,
                    anchor: 'R2',
                    title: '問題R2',
                    subtitle: '',
                    body: `<h4>要讓GenAI能夠回答公司關於ISO 27001內部管理規範的問題，可以使用那種技術來達成？</h4>`,
                    mermaid: null
                },
                {
                    id: 25,
                    anchor: 'A2',
                    title: '問題A2',
                    subtitle: '',
                    body: `<h4>要讓GenAI 自動執行一連串任務，例如收集資料 → 整理摘要 → 寄出報告，應該用哪一種技術？</h4>`,
                    mermaid: null
                },
                {
                    id: 26,
                    anchor: 'A3',
                    title: '問題A3',
                    subtitle: '',
                    body: `<h4>如果我們希望不同 AI 模型（像查資料的 vs 處理數據的）能協作工作，該使用哪種技術？</h4>`,
                    mermaid: null
                },
                {
                    id: 27,
                    anchor: 'V1',
                    title: '問題V1',
                    subtitle: '',
                    body: `<h4>如果一位非工程背景的同仁，用自然語言就可以完成一個小工具，你覺得是用到哪個技術？</h4>`,
                    mermaid: null
                },
                {
                    id: 28,
                    anchor: 'V2',
                    title: '問題V2',
                    subtitle: '',
                    body: `<h4>Business Analyst或Project Manager們可以怎麼來使用這些GenAI技術？</h4>`,
                    mermaid: null
                },
                {
                    id: 29,
                    anchor: 'M2',
                    title: '問題M2',
                    subtitle: '',
                    body: `<h4>請說明Function Calling和MCP之間的差異？</h4>`,
                    mermaid: null
                },
                {
                    id: 30,
                    anchor: 'P2',
                    title: '問題P2',
                    subtitle: '',
                    body: `<h4>你覺得 prompt 裡的角色設定（像是 '你是一個資安顧問'）真的有用嗎？模型會照做？</h4>`,
                    mermaid: null
                }
            ];

            const cardsContainer = document.getElementById('info-cards-container');
            const visualAnchor = document.getElementById('visual-anchor-container');
            const prevBtn = document.getElementById('prev-card');
            const nextBtn = document.getElementById('next-card');
            const startBtn = document.getElementById('start-journey');
            const homeBtn = document.getElementById('go-home');
            const modal = document.getElementById('mermaid-modal');
            const modalOverlay = document.getElementById('modal-overlay');
            const modalCloseBtn = document.getElementById('modal-close');
            const modalTitle = document.getElementById('modal-title');
            const mermaidRenderContainer = document.getElementById('mermaid-render-container');
            const zoomInBtn = document.getElementById('zoom-in');
            const zoomOutBtn = document.getElementById('zoom-out');
            const zoomResetBtn = document.getElementById('zoom-reset');
            const panToggleBtn = document.getElementById('pan-toggle');
            const downloadSvgBtn = document.getElementById('download-svg');
            const downloadPngBtn = document.getElementById('download-png');

            let currentIndex = 0;
            let cardElements = [];
            let zoomLevel = 1.0;
            let panModeEnabled = false;
            let isPanning = false;
            let startX, startY, scrollLeft, scrollTop;

            function generateCards() {
                // Create Home Card (Card 0)
                const homeCard = document.createElement('div');
                homeCard.id = 'card-0';
                homeCard.className = 'card-content bg-white p-8 shadow-lg rounded-lg';
                homeCard.dataset.anchor = 'G';
                
                const homeListItems = cardData.map(card => `<li><a data-card-index="${card.id}">${card.title.split('<')[0].trim()}</a></li>`).join('');
                homeCard.innerHTML = `
                    <h3>探索GenAI應用技術演進</h3>
                    <p>從早期的大型語言模型到現代的多代理系統，見證生成式人工智慧技術的革命性發展歷程。每一項技術都試圖解決前一代的問題，推動著AI向更智能、更實用的方向發展。</p>
                    <ul class="grid grid-cols-2 gap-x-8 home-card-list">${homeListItems}</ul>`;
                cardsContainer.appendChild(homeCard);

                // Create other cards
                cardData.forEach(data => {
                    const cardEl = document.createElement('div');
                    cardEl.id = `card-${data.id}`;
                    cardEl.className = 'card-content bg-white p-8 shadow-lg rounded-lg hidden';
                    cardEl.dataset.anchor = data.anchor;

                    let subtitleHtml = data.subtitle ? `<p class="font-poppins text-sm text-gray-500 mb-4">時間：${data.subtitle}</p>` : '';
                    let diagramHtml = data.mermaid ? `
                        <div class="flex justify-center items-center flex-col mt-6">
                            <i class="fa-solid fa-diagram-project diagram-placeholder"></i>
                            <p class="font-poppins text-sm mt-2">點擊查看圖表</p>
                        </div>` : '';

                    cardEl.innerHTML = `<h3>${data.title}</h3>${subtitleHtml}${data.body}${diagramHtml}`;
                    cardsContainer.appendChild(cardEl);
                });

                cardElements = [homeCard, ...Array.from(cardsContainer.querySelectorAll('.card-content:not(#card-0)'))];
            }
            
            function updateButtons() {
                const totalCards = cardElements.length;
                prevBtn.disabled = currentIndex <= 1;
                nextBtn.disabled = currentIndex >= totalCards - 1;
                prevBtn.classList.toggle('opacity-50', currentIndex <= 1);
                prevBtn.classList.toggle('cursor-not-allowed', currentIndex <= 1);
                nextBtn.classList.toggle('opacity-50', currentIndex >= totalCards - 1);
                nextBtn.classList.toggle('cursor-not-allowed', currentIndex >= totalCards - 1);
            }

            function showCard(index) {
                if (index < 0 || index >= cardElements.length) return;
                
                cardElements.forEach((card, i) => {
                    card.classList.toggle('hidden', i !== index);
                });
                
                visualAnchor.textContent = cardElements[index].dataset.anchor;
                currentIndex = index;
                updateButtons();
            }

            function applyZoom() {
                const svg = mermaidRenderContainer.querySelector('svg');
                if (svg) {
                    svg.style.transform = `scale(${zoomLevel})`;
                }
            }

            function openModal(cardId) {
                const data = cardData.find(c => c.id === cardId);
                if (!data || !data.mermaid) return;

                modalTitle.textContent = data.title.split('<')[0].trim() + ' - 流程圖';
                mermaidRenderContainer.innerHTML = `<div class="mermaid">${data.mermaid}</div>`;
                modal.classList.remove('hidden');
                
                zoomLevel = 1.0;
                applyZoom();
                
                // Disable pan mode when opening modal
                panModeEnabled = false;
                mermaidRenderContainer.classList.remove('pan-active', 'panning');
                panToggleBtn.classList.remove('bg-blue-300');

                mermaid.run({ nodes: mermaidRenderContainer.querySelectorAll('.mermaid') }).then(() => {
                    applyZoom();
                });
            }

            function closeModal() {
                modal.classList.add('hidden');
                mermaidRenderContainer.innerHTML = '';
                zoomLevel = 1.0;
                panModeEnabled = false;
                isPanning = false;
            }

            // Event Listeners
            nextBtn.addEventListener('click', () => showCard(currentIndex + 1));
            prevBtn.addEventListener('click', () => showCard(currentIndex - 1));
            startBtn.addEventListener('click', () => showCard(1));
            homeBtn.addEventListener('click', () => showCard(0));
            modalOverlay.addEventListener('click', closeModal);
            modalCloseBtn.addEventListener('click', closeModal);

            zoomInBtn.addEventListener('click', () => {
                zoomLevel += 0.1;
                applyZoom();
            });
            zoomOutBtn.addEventListener('click', () => {
                zoomLevel = Math.max(0.2, zoomLevel - 0.1);
                applyZoom();
            });
            zoomResetBtn.addEventListener('click', () => {
                zoomLevel = 1.0;
                applyZoom();
            });

            panToggleBtn.addEventListener('click', () => {
                panModeEnabled = !panModeEnabled;
                panToggleBtn.classList.toggle('bg-blue-300', panModeEnabled);
                mermaidRenderContainer.classList.toggle('pan-active', panModeEnabled);
            });

            mermaidRenderContainer.addEventListener('mousedown', (e) => {
                if (!panModeEnabled) return;
                isPanning = true;
                mermaidRenderContainer.classList.add('panning');
                startX = e.pageX - mermaidRenderContainer.offsetLeft;
                startY = e.pageY - mermaidRenderContainer.offsetTop;
                scrollLeft = mermaidRenderContainer.scrollLeft;
                scrollTop = mermaidRenderContainer.scrollTop;
            });

            mermaidRenderContainer.addEventListener('mouseleave', () => {
                if (!panModeEnabled) return;
                isPanning = false;
                mermaidRenderContainer.classList.remove('panning');
            });

            mermaidRenderContainer.addEventListener('mouseup', () => {
                if (!panModeEnabled) return;
                isPanning = false;
                mermaidRenderContainer.classList.remove('panning');
            });

            mermaidRenderContainer.addEventListener('mousemove', (e) => {
                if (!isPanning) return;
                e.preventDefault();
                const x = e.pageX - mermaidRenderContainer.offsetLeft;
                const y = e.pageY - mermaidRenderContainer.offsetTop;
                const walkX = (x - startX) * 2; // Pan speed
                const walkY = (y - startY) * 2;
                mermaidRenderContainer.scrollLeft = scrollLeft - walkX;
                mermaidRenderContainer.scrollTop = scrollTop - walkY;
            });

            downloadSvgBtn.addEventListener('click', () => {
                const svg = mermaidRenderContainer.querySelector('svg');
                if (!svg) return;

                const svgData = new XMLSerializer().serializeToString(svg);
                const blob = new Blob([svgData], { type: 'image/svg+xml;charset=utf-8' });
                const url = URL.createObjectURL(blob);
                const a = document.createElement('a');
                a.href = url;
                a.download = 'mermaid-diagram.svg';
                document.body.appendChild(a);
                a.click();
                document.body.removeChild(a);
                URL.revokeObjectURL(url);
            });

            downloadPngBtn.addEventListener('click', () => {
                const svg = mermaidRenderContainer.querySelector('svg');
                if (!svg) return;

                const canvas = document.createElement('canvas');
                const ctx = canvas.getContext('2d');
                const svgData = new XMLSerializer().serializeToString(svg);
                const img = new Image();

                img.onload = () => {
                    // To get a sharper image, we can render it at a higher resolution
                    const scale = 2;
                    canvas.width = img.width * scale;
                    canvas.height = img.height * scale;
                    ctx.setTransform(scale, 0, 0, scale, 0, 0);
                    ctx.drawImage(img, 0, 0);

                    const pngUrl = canvas.toDataURL('image/png');
                    const a = document.createElement('a');
                    a.href = pngUrl;
                    a.download = 'mermaid-diagram.png';
                    document.body.appendChild(a);
                    a.click();
                    document.body.removeChild(a);
                };

                img.src = 'data:image/svg+xml;base64,' + btoa(unescape(encodeURIComponent(svgData)));
            });

            cardsContainer.addEventListener('click', e => {
                // Home card links
                if (e.target.matches('.home-card-list a')) {
                    e.preventDefault();
                    const index = parseInt(e.target.getAttribute('data-card-index'), 10);
                    showCard(index);
                }
                // Diagram placeholders
                if (e.target.matches('.diagram-placeholder')) {
                    const cardElement = e.target.closest('.card-content');
                    const cardId = parseInt(cardElement.id.replace('card-', ''), 10);
                    openModal(cardId);
                }
            });

            // Initialization
            generateCards();
            showCard(0);
        });
    </script>

</body>
</html>